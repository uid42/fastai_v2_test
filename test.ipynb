{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "#from FLAI.detect_symbol.exp import databunch as databunch_detsym\n",
    "from FLAI.detect_symbol.exp import resnet_ssd as resnet_ssd_detsym\n",
    "from FLAI.detect_symbol.exp import anchors_loss_metrics as anchors_loss_metrics_detsym\n",
    "from FLAI.detect_symbol.exp import optimizer as optimizer_detsym\n",
    "#from FLAI.detect_symbol.exp import init_model as init_model_detsym\n",
    "#from FLAI.detect_symbol.exp import tensorboard_callback\n",
    "#from FLAI.detect_symbol.exp import scheduling_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#最后会引用detect_symbol.databunch，ImageList找不到\n",
    "# sys.path.append('../sick_tree_detection')\n",
    "# from sick_tree_detection.exp import anchors_loss_metrics as anchors_loss_metrics_sicktree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 应对无目标的情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bb_pad_intlbl(samples, pad_idx=0):\n",
    "    \"Function that collect `samples` of labelled bboxes and adds padding with `pad_idx`.\"\n",
    "    samples = [(s[0], *clip_remove_empty(*s[1:])) for s in samples]\n",
    "    max_len = max([len(s[2]) for s in samples])\n",
    "    def _f(img,bbox,lbl):\n",
    "        bbox = torch.cat([bbox,bbox.new_zeros(max_len-bbox.shape[0], 4)])\n",
    "        #lbl  = torch.cat([lbl, lbl .new_zeros(max_len-lbl .shape[0], int)+pad_idx])\n",
    "        #在无目标也就是lbl为[]的情况下，lbl  = torch.cat([lbl, lbl .new_zeros(max_len-lbl .shape[0])+pad_idx])\n",
    "        #上面的代码即使指定了dtype=torch.int得到的仍然是浮点数。会导致后面的报错不是索引\n",
    "        if lbl.shape[0] != 0:\n",
    "            lbl  = torch.cat([lbl, lbl .new_zeros(max_len-lbl .shape[0])+pad_idx])\n",
    "        else:\n",
    "            lbl = lbl.new_zeros(max_len, dtype = torch.int) + pad_idx\n",
    "\n",
    "        \n",
    "        return img,bbox,lbl\n",
    "    return [_f(*s) for s in samples]\n",
    "\n",
    "BBoxBlock = TransformBlock(type_tfms=TensorBBox.create, item_tfms=PointScaler, dls_kwargs = {'before_batch': bb_pad_intlbl})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取BBox和label  \n",
    "两个是分开进行的。并且BBox的顺序改成了先x后y，使用v1版的fastai的数据集的时候需要转换顺序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "pat_coord = re.compile(r'\\d+')\n",
    "pat_clas = re.compile(r'\\w+')\n",
    "pat_imgName = re.compile(r'(\\w+/\\d+\\.png)$')\n",
    "pat_imgName = re.compile(r'(\\w+/\\d+\\.jpg)$')\n",
    "def get_label_from_df(fn, df, pat_imgName, box_col, cat_col):    \n",
    "    fn = str(fn)\n",
    "    pat_cat = re.compile(r'\\w+')\n",
    "    \n",
    "    fn = pat_imgName.findall(str(fn))[0]\n",
    "    cats = df.loc[fn,cat_col]\n",
    "    cats = pat_clas.findall(cats)\n",
    "    \n",
    "    return cats\n",
    "\n",
    "def get_boxes_from_df(fn, df, pat_imgName, box_col, cat_col):\n",
    "    fn = str(fn)\n",
    "    pat_num = re.compile(r'\\d+')\n",
    "    pat_cat = re.compile(r'\\w+')\n",
    "    fn = pat_imgName.findall(str(fn))[0]\n",
    "    #print('dbg1', fn)\n",
    "    \n",
    "    boxes = df.loc[fn,box_col]\n",
    "    boxes = pat_num.findall(boxes)\n",
    "    #boxes = list(map(np.long, boxes))\n",
    "    boxes = list(map(np.int32, boxes))\n",
    "    boxes = np.array(boxes).reshape(-1,4)\n",
    "    \n",
    "    #fastai2里面bbox的顺序改成了xy的顺序。现在用的这个数据集还是v1里面的yx的顺序。这里调整一下\n",
    "    boxes = boxes[...,[1, 0, 3, 2]]\n",
    "    boxes = boxes.tolist()\n",
    "    \n",
    "    cats = df.loc[fn,cat_col]\n",
    "    cats = pat_clas.findall(cats)\n",
    "    #print('dbg2', fn, boxes, cats)\n",
    "    assert len(boxes)==len(cats), 'length of bounding boxes and categories not equeal.'\n",
    "    \n",
    "    #print('dbg_boxes:', boxes)    \n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成DataBlock\n",
    "作用相当于之前的DataBunch  \n",
    "item_tfms=Resize(128) 作用类似v1里面的after_open，可以对图片进行一些处理，但是这个处理无法作用在y上,如果需要改变图片尺寸连带y一起改变，应该在aug_transforms里面指定size参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_db():\n",
    "    get_y1 = partial(get_boxes_from_df, df=df, pat_imgName=pat_imgName, box_col='box', cat_col='cls')\n",
    "    get_y2 = partial(get_label_from_df, df=df, pat_imgName=pat_imgName, box_col='box', cat_col='cls')\n",
    "    \n",
    "    syms = DataBlock(blocks=(ImageBlock, BBoxBlock, BBoxLblBlock),\n",
    "                     get_items=get_image_files,\n",
    "                     splitter=RandomSplitter(),\n",
    "                     get_y=[get_y1, get_y2],\n",
    "                     #item_tfms=Resize(128),\n",
    "                     #batch_tfms=aug_transforms(size=(128,128)),\n",
    "                     n_inp=1)\n",
    "    return syms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtValidCal(TrainEvalCallback):\n",
    "    def before_fit(self):\n",
    "        import pdb;pdb.set_trace()\n",
    "        self.recorder.add_metric_names(['ext_valid'])\n",
    "    def after_epoch(self, **kwargs):\n",
    "        import pdb;pdb.set_trace()\n",
    "        print('on after epoch', kwargs)\n",
    "    '''def __getattr__(self,k): \n",
    "        import pdb;pdb.set_trace()\n",
    "        if k in ['ext_valids', 'fld_names']:\n",
    "            return self.ext_valids\n",
    "        else:\n",
    "            return getattr(self.learn, k)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 网络-病树检测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ssd_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ssd_block(nn.Module):\n",
    "    '''\n",
    "    和detect_symbol里面的ssd_block相比只是去掉了宽高相关的部分\n",
    "    '''\n",
    "    def __init__(self, k, nin, n_clas):\n",
    "        '''\n",
    "        ssd头模块，它根据某层的特征图给出bbox预测信息，该模块的输出包含4个部分：\n",
    "        -- loc：bbox中心偏移，2个值\n",
    "        -- conf：目标信心，1个值\n",
    "        -- clas：目标类别，n_clas个值\n",
    "        ----------------------------------------\n",
    "        参数：\n",
    "        -- k：每个grid的anchor数\n",
    "        -- nin：输入特征图通道数\n",
    "        -- n_clas：目标类别数\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.oconv_loc = nn.Conv2d(nin, 2*k, 3, padding=1) # bbox center\n",
    "        self.oconv_conf = nn.Conv2d(nin, 1*k, 3, padding=1) # confidence\n",
    "        self.oconv_clas = nn.Conv2d(nin, n_clas*k, 3, padding=1) # classification\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return (resnet_ssd_detsym.flatten_grid_anchor(self.oconv_loc(x), self.k),\n",
    "                resnet_ssd_detsym.flatten_grid_anchor(self.oconv_conf(x), self.k),\n",
    "                resnet_ssd_detsym.flatten_grid_anchor(self.oconv_clas(x), self.k)\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNetIsh_1SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetIsh_1SSD(resnet_ssd_detsym.ResNetIsh_1SSD):    \n",
    "    def forward(self, x):\n",
    "        outs = self._forward_impl(x)\n",
    "        \n",
    "        locs,confs,clss = [],[],[]\n",
    "        for out in outs:\n",
    "            locs += [out[0]]\n",
    "            confs += [out[1]]\n",
    "            clss += [out[2]]\n",
    "        \n",
    "        return (torch.cat(locs,dim=1),\n",
    "                torch.cat(confs,dim=1),\n",
    "                torch.cat(clss,dim=1)\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet18_1ssd(layers4fpn = False, num_classes = 1):\n",
    "    #layers4fpn是否保留后面的两层给fpn用\n",
    "    if not layers4fpn:\n",
    "        return ResNetIsh_1SSD(block=torchvision.models.resnet.BasicBlock,\n",
    "                   layers=[2,2,2],\n",
    "                   chs=[64,128,256],\n",
    "                   strides=[1,2,2],\n",
    "                   pred_layerIds=[2],\n",
    "                   num_anchors=1,\n",
    "                   neck_block=resnet_ssd_detsym.cnv1x1_bn_relu,\n",
    "                   head_chin=256,\n",
    "                   head_block=ssd_block,\n",
    "                   num_classes=num_classes)\n",
    "    else:\n",
    "        assert False, '没有实现'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### anchor_loss_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_2020904_ = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_neibs(idx, grids = (49, 49), dis = 1):\n",
    "    '''\n",
    "    找到某个anchor周围相邻的anchors的下标里列表。距离默认1。\n",
    "    这个任务中只有第一层的grids参与，所以只需要第一次的grids的尺寸。\n",
    "    anchor也是1对1的。\n",
    "    参数：\n",
    "        idx：目标anchor在grid anchors(get_grid_anchors返回的gvs)列表中的下标\n",
    "        grids: 尺寸\n",
    "        dis：邻居的距离\n",
    "    返回值：\n",
    "        邻居的下标列表\n",
    "    '''\n",
    "    if TEST_2020904_:\n",
    "        dis = 5\n",
    "        \n",
    "    gh, gw = grids\n",
    "    x = idx % gh\n",
    "    y = idx // gw\n",
    "    ret = []\n",
    "    for nx in range(x - dis, x + dis + 1):\n",
    "        for ny in range(y - dis, y + dis + 1):\n",
    "            if nx >= 0 and ny >= 0 and nx < gw and ny < gh \\\n",
    "                    and not(nx == x and ny == y):\n",
    "                nidx = ny * gw + nx\n",
    "                ret += [nidx]\n",
    "    return ret      \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y(pts):\n",
    "    keep = pts.abs().sum(-1).nonzero()[:,0]\n",
    "    return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义一个新的GridAnchor_Functions，主要是修改:\n",
    "#get_scroe_hits->get_hits,b2t->b2c,t2b->c2b;\n",
    "#LblPts指定是使用ImageBBox还是labled points\n",
    "class GridAnchor_Funcs(anchors_loss_metrics_detsym.GridAnchor_Funcs):\n",
    "    def __init__(self, fig_hw, grids, device, LblPts = True):\n",
    "        anchors = [[(0, 0)]]\n",
    "        gvs,ghs,gws,avs,ahs,aws = anchors_loss_metrics_detsym.get_grids_anchors( \\\n",
    "                    fig_hw, grids, anchors)\n",
    "        self.grids = grids\n",
    "        self.LblPts = LblPts\n",
    "        super().__init__(gvs, avs, device)\n",
    "        \n",
    "    #下面的三个函数都用不上了。防止被调用到。\n",
    "    def get_scores_hits(self, gt_bb_or_lpts): \n",
    "        assert False, 'deleted'\n",
    "    def b2t(self, gt_bb_or_lpts,idx,eps=1):\n",
    "        assert False, 'deleted'\n",
    "    def t2b(self,t,idx,eps=1):\n",
    "        assert False, 'deleted'\n",
    "        \n",
    "    def get_hits(self, gt_bb_or_lpts): \n",
    "        # ground truch bbox center x,y\n",
    "        if not self.LblPts:\n",
    "            gt_cx = gt_bb_or_lpts[:,[0,2]].mean(-1)\n",
    "            gt_cy = gt_bb_or_lpts[:,[1,3]].mean(-1)\n",
    "        else:\n",
    "            gt_cx = gt_bb_or_lpts[:,[0]].mean(-1)\n",
    "            gt_cy = gt_bb_or_lpts[:,[1]].mean(-1)\n",
    "\n",
    "        # 判断目标bbox的中心落在哪个cell内\n",
    "        hits = ((gt_cx[:,None] >= self.gvs[:,0][None]) &\n",
    "                (gt_cx[:,None] <  self.gvs[:,2][None]) &\n",
    "                (gt_cy[:,None] >= self.gvs[:,1][None]) &\n",
    "                (gt_cy[:,None] <  self.gvs[:,3][None]))\n",
    "        \n",
    "        return hits\n",
    "   \n",
    "    def b2c(self, gt_bb_or_lpts,idx,eps=1):\n",
    "        '''\n",
    "        gt_bb_or_lpts->center        \n",
    "        '''\n",
    "        cx,cy = self.gvs[idx,0],self.gvs[idx,1]\n",
    "        gh,gw = self.ghs[idx],self.gws[idx]\n",
    "        #ph,pw = self.ahs[idx],self.aws[idx]\n",
    "\n",
    "        if not self.LblPts:\n",
    "            bx = (gt_bb_or_lpts[:,0] + gt_bb_or_lpts[:,2])/2 # x of center of box\n",
    "            by = (gt_bb_or_lpts[:,1] + gt_bb_or_lpts[:,3])/2 # y of center of box\n",
    "        else:\n",
    "            bx = gt_bb_or_lpts[:,0]\n",
    "            by = gt_bb_or_lpts[:,1]\n",
    "        hatsig_tx = (bx - cx)/gh\n",
    "        hatsig_ty = (by - cy)/gw\n",
    "        \n",
    "        sig_tx = (hatsig_tx+0.5*eps)/(1+eps)\n",
    "        sig_ty = (hatsig_ty+0.5*eps)/(1+eps)\n",
    "\n",
    "        tx = torch.log(sig_tx/(1-sig_tx))\n",
    "        ty = torch.log(sig_ty/(1-sig_ty))\n",
    "        \n",
    "        return torch.stack([tx, ty]).t()\n",
    "  \n",
    "    def c2b(self,t,idx,eps=1):\n",
    "        '''\n",
    "        center->gt_bb_or_lpts.\n",
    "            如果是ImageBBox那么这些bbox都是没有宽高的。也就是后右下角坐标和左上角坐标相同。\n",
    "            或者是Points\n",
    "        '''\n",
    "        cx,cy = self.gvs[idx,0],self.gvs[idx,1]\n",
    "        gh,gw = self.ghs[idx],self.gws[idx]\n",
    "\n",
    "        sig_tx = torch.sigmoid(t[...,0])\n",
    "        sig_ty = torch.sigmoid(t[...,1])\n",
    "        \n",
    "        hatsig_tx = (1+eps)*(sig_tx-0.5) + 0.5\n",
    "        hatsig_ty = (1+eps)*(sig_ty-0.5) + 0.5\n",
    "\n",
    "        bx = hatsig_tx*gw + cx # x of center of box\n",
    "        by = hatsig_ty*gh + cy # y of center of box\n",
    "        \n",
    "        if not self.LblPts:\n",
    "            res = torch.stack([bx, by, bx, by],dim=0)\n",
    "        else:\n",
    "            res = torch.stack([bx, by],dim=0)\n",
    "        res = res.permute(list(range(len(res.shape)))[1:]+[0])\n",
    "        return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clas_acc(pred_batch, *gt_batch, gaf):\n",
    "    '''\n",
    "    classification accuracy\n",
    "    '''\n",
    "    posCnt = tensor(0.)\n",
    "    totCnt = tensor(0.)\n",
    "    for pred_clas,gt_bb_or_lpts,gt_clas in zip(pred_batch[2], *gt_batch):\n",
    "        if not gaf.LblPts:\n",
    "            keep = anchors_loss_metrics_detsym.get_y(gt_bb_or_lpts)\n",
    "        else:\n",
    "            keep = get_y(gt_bb_or_lpts)\n",
    "        if keep.numel()==0: continue\n",
    "        \n",
    "        gt_bb_or_lpts = gt_bb_or_lpts[keep]\n",
    "        gt_clas = gt_clas[keep]\n",
    "        \n",
    "        gt_bb_or_lpts = (gt_bb_or_lpts + 1) / 2\n",
    "        gt_clas = gt_clas - 1 # the databunch add a 'background' class to classes[0], but we don't want that,so gt_clas-1\n",
    "        \n",
    "        hits = gaf.get_hits(gt_bb_or_lpts)\n",
    "        idx = idx_fromHits(hits)\n",
    "        \n",
    "        pred_clas = pred_clas[idx]\n",
    "        pred_clas = pred_clas.max(1)[1]\n",
    "        \n",
    "        posCnt += (pred_clas==gt_clas).sum().item()\n",
    "        totCnt += gt_clas.shape[0]\n",
    "\n",
    "    return posCnt/totCnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clas_L(pred_batch, *gt_batch, lambda_clas=1, clas_weights=None, gaf):\n",
    "    '''\n",
    "    class loss\n",
    "    若某anchor对某object负责，则应训练其classification靠近该object的类别。\n",
    "    '''\n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    for pred_clas,gt_bb_or_lpts,gt_clas in zip(pred_batch[2], *gt_batch):\n",
    "        if gaf.LblPts:\n",
    "            keep = get_y(gt_bb_or_lpts)\n",
    "        else:\n",
    "            keep = anchors_loss_metrics_detsym.get_y(gt_bb_or_lpts)\n",
    "        if keep.numel()==0: continue\n",
    "        \n",
    "        gt_bb_or_lpts = gt_bb_or_lpts[keep]\n",
    "        gt_clas = gt_clas[keep]\n",
    "        \n",
    "        gt_bb_or_lpts = (gt_bb_or_lpts + 1) / 2\n",
    "        gt_clas = gt_clas - 1 # the databunch add a 'background' class to classes[0], but we don't want that,so gt_clas-1\n",
    "        \n",
    "        hits = gaf.get_hits(gt_bb_or_lpts)\n",
    "        idx = idx_fromHits(hits)\n",
    "        \n",
    "        pred_clas = pred_clas[idx]\n",
    "        \n",
    "        loss += F.cross_entropy(pred_clas, gt_clas, weight=clas_weights, reduction='sum')\n",
    "        cnt += gt_clas.shape[0]\n",
    "        \n",
    "    return lambda_clas*loss/cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cent_L(pred_batch, *gt_batch, lambda_cent=1, clas_weights=None, gaf):\n",
    "    '''\n",
    "    bbox center loss\n",
    "    若某 anchor 对某 object 负责，则应训练其预测之 中心 靠近该 object box 之 中心。\n",
    "    '''\n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    for pred_txy,gt_bb_or_lpts,gt_clas in zip(pred_batch[0], *gt_batch):\n",
    "        if not gaf.LblPts:\n",
    "            keep = anchors_loss_metrics_detsym.get_y(gt_bb_or_lpts)\n",
    "        else:\n",
    "            keep = get_y(gt_bb_or_lpts)\n",
    "        if keep.numel()==0: continue\n",
    "          \n",
    "        gt_bb_or_lpts = gt_bb_or_lpts[keep]\n",
    "        gt_clas = gt_clas[keep]\n",
    "        \n",
    "        gt_bb_or_lpts = (gt_bb_or_lpts + 1) / 2\n",
    "        gt_clas = gt_clas - 1\n",
    "        \n",
    "        if clas_weights is not None: ws = clas_weights[gt_clas]\n",
    "        else: ws = None\n",
    "        \n",
    "        hits = gaf.get_hits(gt_bb_or_lpts)\n",
    "        idx = idx_fromHits(hits)\n",
    "        \n",
    "        gt_t = gaf.b2c(gt_bb_or_lpts,idx,eps=1)\n",
    "        pred_txy = pred_txy[idx]\n",
    "        \n",
    "        if ws is not None:\n",
    "            tmp = ((gt_t[...,:2]-pred_txy)*ws[...,None]).abs().sum()\n",
    "        else:\n",
    "            tmp = (gt_t[...,:2]-pred_txy).abs().sum()\n",
    "        \n",
    "        loss += tmp\n",
    "        cnt += len(idx)\n",
    "    \n",
    "    return lambda_cent*loss/cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pConf_L(pred_batch, *gt_batch, lambda_pconf=1, clas_weights=None, gaf):\n",
    "    '''\n",
    "    positive confidence loss\n",
    "    若某 anchor 为某 object 负责，则训练其 conf_score 靠近 1。\n",
    "    '''\n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    for pred_conf,gt_bb_or_lpts,gt_clas in zip(pred_batch[1], *gt_batch):\n",
    "        if not gaf.LblPts:\n",
    "            keep = anchors_loss_metrics_detsym.get_y(gt_bb_or_lpts)\n",
    "        else:\n",
    "            keep = get_y(gt_bb_or_lpts)\n",
    "            \n",
    "        if keep.numel()==0: continue\n",
    "          \n",
    "        gt_bb_or_lpts = gt_bb_or_lpts[keep]\n",
    "        gt_clas = gt_clas[keep]\n",
    "        \n",
    "        gt_bb_or_lpts = (gt_bb_or_lpts + 1) / 2\n",
    "        gt_clas = gt_clas - 1\n",
    "        \n",
    "        if clas_weights is not None: ws = clas_weights[gt_clas]\n",
    "        else: ws = None\n",
    "        \n",
    "        hits = gaf.get_hits(gt_bb_or_lpts)\n",
    "        idx = idx_fromHits(hits)\n",
    "        \n",
    "        conf_pos = pred_conf[idx]\n",
    "#         conf_pos = torch.sigmoid(conf_pos)\n",
    "#         tmp = (1-conf_pos).abs().sum()\n",
    "        if ws is not None: \n",
    "            tmp = F.binary_cross_entropy_with_logits(conf_pos,torch.ones_like(conf_pos),weight=ws[...,None],reduction='sum')\n",
    "        else: \n",
    "            tmp = F.binary_cross_entropy_with_logits(conf_pos,torch.ones_like(conf_pos),reduction='sum')\n",
    "    \n",
    "    \n",
    "        loss += tmp\n",
    "        cnt += len(idx)\n",
    "        \n",
    "    return lambda_pconf*loss/cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nConf_L(pred_batch, *gt_batch, gaf, conf_th=0.5, lambda_nconf=1):\n",
    "    '''\n",
    "    negative confidence loss\n",
    "    若某 anchor 不对任何 object 负责，且它与任何 object 的 匹配得分 都差于 threshold，则训练其 conf_score 靠近 0。\n",
    "    '''\n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    for pred_conf,gt_bb_or_lpts,_ in zip(pred_batch[1], *gt_batch):\n",
    "        if not gaf.LblPts:\n",
    "            keep = anchors_loss_metrics_detsym.get_y(gt_bb_or_lpts)\n",
    "        else:\n",
    "            keep = get_y(gt_bb_or_lpts)\n",
    "        if keep.numel()==0: continue\n",
    "        \n",
    "        gt_bb_or_lpts = gt_bb_or_lpts[keep]\n",
    "        gt_bb_or_lpts = (gt_bb_or_lpts + 1) / 2\n",
    "        \n",
    "        hits = gaf.get_hits(gt_bb_or_lpts)\n",
    "        idx = idx_fromHits(hits)\n",
    "        \n",
    "        #positive\n",
    "        tmp = (hits * 1).max(dim=0)[0]\n",
    "        \n",
    "        #取得命中的anchor周围的anchor的下标立标\n",
    "        discards = []\n",
    "        for hidx in idx:\n",
    "            neibs = find_neibs(hidx, gaf.grids[0], dis = 1)            \n",
    "            for i in neibs:\n",
    "                discards += [i]\n",
    "        #把周围的邻居加进来，剩下的就是negative了\n",
    "        tmp[discards] = 1\n",
    "        \n",
    "        neg_idx = torch.where(tmp==0)[0] # 如果没有，该anchor是negative anchor\n",
    "        \n",
    "        conf_neg = pred_conf[neg_idx]\n",
    "#         conf_neg = torch.sigmoid(conf_neg)\n",
    "#         loss += conf_neg.abs().sum()\n",
    "        tmp = F.binary_cross_entropy_with_logits(conf_neg,torch.zeros_like(conf_neg),reduction='sum')\n",
    "        loss += tmp\n",
    "        cnt += len(neg_idx)\n",
    "        \n",
    "    return lambda_nconf*loss/cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_L(pred_batch, *gt_batch, conf_th=0.5,\n",
    "           lambda_cent=1, lambda_pconf=1, lambda_nconf=1, lambda_clas=1, clas_weights=None, gaf):\n",
    "    '''\n",
    "    与detect_symbol里面的yolo_L相比的区别是：\n",
    "        不计算宽高方面的损失\n",
    "        neg_idx要去掉find_neibs返回的discard列表\n",
    "        \n",
    "    clas_weights: \n",
    "    为了解决数据集的imbalance问题，一种方法是在dataloader中使用WeightedRandomSampler，但是这种方法不适用于目标检测问题。\n",
    "    因为，（1）目标检测的label不是一个简单的数值（2）目标检测问题的一张图片可能包括不同类别的多个目标。\n",
    "    所以为了解决目标检测问题中的imbalance问题，我们的方法是在损失函数中使用权重。\n",
    "    为各类别分配权重，各目标对应的损失乘以该目标所属类别的权重。\n",
    "    默认为None，即不使用权重。\n",
    "    若设置非None，则clas_weights应该是一个一维tensor，其长度等于数据集的类别数。\n",
    "    若设置为全1，则相当于不使用权重。\n",
    "    合理的设置应保证所有元素之和等于数据集的类别数，否则相当于对损失函数的整体做了缩放。\n",
    "    '''\n",
    "    clas_loss = 0\n",
    "    cent_loss = 0\n",
    "    pconf_loss = 0\n",
    "    nconf_loss = 0\n",
    "    pos_cnt = 0\n",
    "    neg_cnt = 0\n",
    "    \n",
    "    for pred_txy,pred_conf,pred_clas,gt_bb_or_lpts,gt_clas in zip(*pred_batch, *gt_batch):\n",
    "        if not gaf.LblPts:\n",
    "            keep = anchors_loss_metrics_detsym.get_y(gt_bb_or_lpts)\n",
    "        else:\n",
    "            keep = get_y(gt_bb_or_lpts)\n",
    "        if keep.numel()==0: \n",
    "            #这时候所有anchor都是negative的。所以空白的也要贡献自己的loss\n",
    "            conf_neg = pred_conf#所有anchor的\n",
    "            nconf_loss += F.binary_cross_entropy_with_logits(conf_neg,torch.zeros_like(conf_neg),reduction='sum')\n",
    "            neg_cnt += len(pred_conf)\n",
    "            continue\n",
    "          \n",
    "        gt_bb_or_lpts = gt_bb_or_lpts[keep]\n",
    "        gt_clas = gt_clas[keep]\n",
    "        \n",
    "        gt_bb_or_lpts = (gt_bb_or_lpts + 1) / 2\n",
    "        gt_clas = gt_clas - 1 # the databunch add a 'background' class to classes[0], but we don't want that,so gt_clas-1\n",
    "        \n",
    "        if clas_weights is not None: ws = clas_weights[gt_clas]\n",
    "        else: ws = None\n",
    "        \n",
    "        hits = gaf.get_hits(gt_bb_or_lpts)\n",
    "        idx = idx_fromHits(hits)\n",
    "        \n",
    "        # classification loss\n",
    "        pred_clas = pred_clas[idx]\n",
    "        clas_loss += F.cross_entropy(pred_clas, gt_clas, weight=clas_weights, reduction='sum')\n",
    "        \n",
    "        # bbox center loss\n",
    "        gt_t = gaf.b2c(gt_bb_or_lpts,idx,eps=1)\n",
    "        pred_txy = pred_txy[idx]\n",
    "        if ws is not None:\n",
    "            cent_loss += ((gt_t[...,:2]-pred_txy)*ws[...,None]).abs().sum()\n",
    "        else:\n",
    "            cent_loss += (gt_t[...,:2]-pred_txy).abs().sum()\n",
    "        \n",
    "        # positive confidence loss\n",
    "        conf_pos = pred_conf[idx]\n",
    "        if ws is not None: \n",
    "            pconf_loss += F.binary_cross_entropy_with_logits(conf_pos,torch.ones_like(conf_pos),weight=ws[...,None],reduction='sum')\n",
    "        else: \n",
    "            pconf_loss += F.binary_cross_entropy_with_logits(conf_pos,torch.ones_like(conf_pos),reduction='sum')\n",
    "\n",
    "        #positive\n",
    "        tmp = (hits * 1).max(dim=0)[0]\n",
    "        \n",
    "        #取得命中的anchor周围的anchor的下标立标\n",
    "        discards = []\n",
    "        for hidx in idx:\n",
    "            neibs = find_neibs(hidx, gaf.grids[0], dis = 1)            \n",
    "            for i in neibs:\n",
    "                discards += [i]\n",
    "        #把周围的邻居加进来，剩下的就是negative了\n",
    "        tmp[discards] = 1\n",
    "         \n",
    "        neg_idx = torch.where(tmp==0)[0] # 如果没有，该anchor是negative anchor\n",
    "        \n",
    "        conf_neg = pred_conf[neg_idx]\n",
    "        nconf_loss += F.binary_cross_entropy_with_logits(conf_neg,torch.zeros_like(conf_neg),reduction='sum')\n",
    "        \n",
    "        pos_cnt += len(idx)\n",
    "        neg_cnt += len(neg_idx)\n",
    "        \n",
    "    \n",
    "    if pos_cnt > 0:#测试的极端情况碰到都是空白的。只有nconf_loss在前面计算了。\n",
    "        clas_loss  = lambda_clas  * clas_loss  /pos_cnt\n",
    "        cent_loss  = lambda_cent  * cent_loss  /pos_cnt\n",
    "        pconf_loss = lambda_pconf * pconf_loss /pos_cnt\n",
    "    nconf_loss = lambda_nconf * nconf_loss /neg_cnt\n",
    "    \n",
    "    return clas_loss + cent_loss + pconf_loss + nconf_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox2c(b):\n",
    "    '''\n",
    "    将bbox的（左上x，左上y，右下x，右下y）表示变为（中心x，中心y）表示\n",
    "    '''\n",
    "    cx = b[...,[0,2]].mean(-1)[...,None]\n",
    "    cy = b[...,[1,3]].mean(-1)[...,None]\n",
    "    \n",
    "    return torch.cat([cx,cy],dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_fromHits(hits):\n",
    "    idx = (hits * 1).max(1)[1]\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cent_d(pred_batch, *gt_batch, gaf):\n",
    "    '''\n",
    "    bbox center difference\n",
    "    '''\n",
    "    dif = tensor(0.)\n",
    "    cnt = tensor(0.)\n",
    "    for pred_txy,gt_bb_or_lpts,_ in zip(pred_batch[0], *gt_batch):\n",
    "        if not gaf.LblPts:\n",
    "            keep = anchors_loss_metrics_detsym.get_y(gt_bb_or_lpts)\n",
    "        else:\n",
    "            keep = get_y(gt_bb_or_lpts)\n",
    "        if keep.numel()==0: continue\n",
    "          \n",
    "        #pred_t = torch.cat([pred_txy,pred_thw],dim=1)\n",
    "        pred_t = pred_txy\n",
    "        \n",
    "        gt_bb_or_lpts = gt_bb_or_lpts[keep]\n",
    "        gt_bb_or_lpts = (gt_bb_or_lpts + 1) / 2\n",
    "        \n",
    "        hits = gaf.get_hits(gt_bb_or_lpts)\n",
    "        idx = idx_fromHits(hits)\n",
    "        \n",
    "        pred_t = pred_t[idx]\n",
    "        if not gaf.LblPts:\n",
    "            pred_c = bbox2c(gaf.c2b(pred_t,idx))[...,:2]\n",
    "            gt_c = bbox2c(gt_bb_or_lpts)[...,:2]\n",
    "        else:\n",
    "            pred_c = gaf.c2b(pred_t,idx)\n",
    "            gt_c = gt_bb_or_lpts\n",
    "        \n",
    "        tmp = (gt_c - pred_c).abs().sum()\n",
    "        dif += tmp\n",
    "        cnt += len(idx)\n",
    "    \n",
    "    return dif/cnt/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_model(model):\n",
    "#     idb.set_trace()\n",
    "    group0 = ModuleList()\n",
    "    group1 = ModuleList()\n",
    "    \n",
    "    pretrained_layers = Sequential(model.conv1, model.bn1, model.res_blocks[:4])\n",
    "#     noPretrain_layers = Sequential(model.res_blocks[4], model.neck_blocks, model.head_block)\n",
    "    noPretrain_layers = Sequential(model.neck_blocks, model.head_block)\n",
    "    \n",
    "    #把pretrained layers分作batchnorm部分（放在group1），和非batchnorm部分（放在group0）\n",
    "    for m in pretrained_layers.modules():\n",
    "        if isinstance(m,bn_types): group1.append(m)\n",
    "        elif isinstance(m,bias_types): group0.append(m)\n",
    "            \n",
    "    #把非pretrain的层放到group1\n",
    "    for m in noPretrain_layers.children():\n",
    "        group1.append(m)\n",
    "    \n",
    "    return [group0, group1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = '/home/dev/jupyter/detect_symbol/data/ds_20200429/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = src_path + 'images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(src_path + 'gends.csv',index_col=0)\n",
    "df = df.set_index('image')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syms = get_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在docker中如果没有设置-shm-size，不设置num_workers=0会使用_MultiProcessingDataLoaderIter，导致错误： \n",
    "Unable to write to file </torch_18692_1954506624>\n",
    "https://discuss.pytorch.org/t/unable-to-write-to-file-torch-18692-1954506624/9990\n",
    "\n",
    "在fastai v1中对应的错误是内存溢出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dls = syms.dataloaders(path, bs = 16, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dls.show_batch(max_n = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#syms.summary(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dts = syms.datasets(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.loc['images/02364.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型和训练-符号检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet_ssd_detsym.get_resnet34_1ssd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gvs,_,_,avs,_,_ = anchors_loss_metrics_detsym.get_ga666()\n",
    "gaf = anchors_loss_metrics_detsym.GridAnchor_Funcs(gvs,avs,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = partial(anchors_loss_metrics_detsym.yolo_L, gaf=gaf, conf_th=1, clas_weights=None, lambda_nconf=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evc = ExtValidCal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn = cnn_learner(dls, model, pretrained=False)\n",
    "learn = Learner(dls, model, loss_func = loss_func, device = device)#, metrics = [evc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evc.learn = learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用病树检测的模型试一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_resnet18_1ssd(num_classes = 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load('../sick_tree_detection/models/pretrained_res18_1ssd_detsym17clas.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../sick_tree_detection/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaf = GridAnchor_Funcs(fig_hw = (776,776)\n",
    "                         , grids = [(49,49)]\n",
    "                         , device = device)\n",
    "gvs, avs = gaf.gvs, gaf.avs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clas_cnts = [11191, 712, 1362, 224, 8710, 1212, 1139, 8686, 857, 2176, 6175, 1869, 14794, 1435, 13628, 9618, 1462]\n",
    "weights = anchors_loss_metrics_detsym.get_clasWeights(clas_cnts,10)\n",
    "weights = tensor(weights).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = partial(yolo_L, gaf=gaf, conf_th=1, clas_weights=None, lambda_nconf=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, model, loss_func = loss_func, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dbg():\n",
    "    import pdb;pdb.set_trace()\n",
    "    #dls.show_batch()\n",
    "    #dls = syms.dataloaders(path)\n",
    "    syms2 = DataBlock(blocks=(ImageBlock, BBoxBlock, BBoxLblBlock),\n",
    "                 get_items=get_image_files,\n",
    "                 splitter=RandomSplitter(),\n",
    "                 get_y=[get_y1, get_y2],\n",
    "                 item_tfms=Resize(128),\n",
    "                 #batch_tfms=aug_transforms(),\n",
    "                 n_inp=1)\n",
    "    dls = syms2.dataloaders(path)\n",
    "    dls.show_batch()\n",
    "dbg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_source = untar_data(URLs.COCO_TINY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, lbl_bbox = get_annotations(coco_source/'train.json')\n",
    "img2bbox = dict(zip(images, lbl_bbox))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y1(o):\n",
    "    #print('dbg0', o)\n",
    "    #return [img2bbox[o.name][0], img2bbox[o.name][1]]\n",
    "    return img2bbox[o.name][0]\n",
    "\n",
    "def get_y2(o): \n",
    "    return img2bbox[o.name][1]\n",
    "\n",
    "coco = DataBlock(blocks=(ImageBlock, BBoxBlock, BBoxLblBlock),\n",
    "                 get_items=get_image_files,\n",
    "                 splitter=RandomSplitter(),\n",
    "                 get_y=[get_y1, get_y2], #[lambda o: img2bbox[o.name][0], lambda o: img2bbox[o.name][1]]\n",
    "                 item_tfms=Resize(128),\n",
    "                 batch_tfms=aug_transforms(),\n",
    "                 n_inp=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cocodls = coco.dataloaders('/root/.fastai/data/coco_tiny')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cocodls.show_batch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pets = DataBlock(blocks = (ImageBlock, CategoryBlock),\n",
    "#                  get_items=get_image_files, \n",
    "#                  splitter=RandomSplitter(seed=42),\n",
    "#                  get_y=using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'),\n",
    "#                  item_tfms=Resize(460),\n",
    "#                  batch_tfms=aug_transforms(size=224, min_scale=0.75))\n",
    "# dls = pets.dataloaders(path/\"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
